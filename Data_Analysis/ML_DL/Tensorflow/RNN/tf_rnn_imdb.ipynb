{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "tf_rnn12imdb.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "name": "pycharm-2419e99e",
   "language": "python",
   "display_name": "PyCharm (Study)"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "jf9U_VpSTcIf"
   },
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "np.random.seed(3)\n",
    "tf.random.set_seed(3)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "s2htOFvWTwOJ"
   },
   "source": [
    "print(imdb.load_data())\n",
    "vocab_size = 10000\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)     # (25000,) (25000,) (25000,) (25000,)\n",
    "\n",
    "print(y_train[:3])\n",
    "num_classes = max(y_train) + 1\n",
    "print(\"num_classes : \", num_classes)\n",
    "print(set(y_train), ' ', np.unique(y_train))\n",
    "\n",
    "print(x_train[0])\n",
    "print(y_train[0])\n",
    "\n",
    "# 시각화 : 훈련용 리뷰 분포 \n",
    "len_result = [len(s) for s in x_train]\n",
    "print('리뷰 최대 길이 : ', np.max(len_result))      # 2494\n",
    "print('리뷰 평균 길이 : ', np.mean(len_result))     # 238.7\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.boxplot(len_result)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(len_result, bins=50)\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2c7Q90QkZt-e"
   },
   "source": [
    "# 긍/부정 빈도수\n",
    "unique_ele, counts_ele = np.unique(y_train, return_counts=True)\n",
    "print(np.asarray((unique_ele, counts_ele)))\n",
    "\n",
    "# index에 대한 단어 출력\n",
    "word_to_index = imdb.get_word_index()\n",
    "index_to_word = {v:k for k, v in word_to_index.items()}\n",
    "print(index_to_word)\n",
    "\n",
    "print(index_to_word)\n",
    "print(index_to_word[1])\n",
    "print(index_to_word[1408])\n",
    "\n",
    "print(x_train[0])\n",
    "print(' '.join(index_to_word[index] for index in x_train[0]))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VP2BFMuWbPpi"
   },
   "source": [
    "# LSTM으로 감성분류 \n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "max_len=500\n",
    "x_train = pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = pad_sequences(x_test, maxlen=max_len)\n",
    "print(x_train[0].shape)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 120))\n",
    "model.add(LSTM(120, activation='tanh'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "es = EarlyStopping(monitor='val_loss', mode='auto', patience=3, baseline=0.01)\n",
    "mc = ModelCheckpoint('tfrnn12.h5', monitor='val_acc', mode='max', save_best_only=True)\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=100, batch_size=64, verbose=2, callbacks=[es, mc])\n",
    "\n",
    "loaded_model = load_model('tfrnn12.h5')\n",
    "print('acc : ', loaded_model.evaluate(x_test, y_test)[1])\n",
    "print('loss : ', loaded_model.evaluate(x_test, y_test)[0])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "McFDkrYv4kFf"
   },
   "source": [
    "# CNN 으로 텍스트 분류\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPool1D, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 256))\n",
    "model.add(Conv1D(256,  kernel_size=3, padding='valid', activation='relu', strides=1))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "es = EarlyStopping(monitor='val_loss', mode='auto', patience=3, baseline=0.01)\n",
    "mc = ModelCheckpoint('tfrnn12_1.h5', monitor='val_acc', mode='max', save_best_only=True)\n",
    "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=100, batch_size=64, verbose=2, callbacks=[es, mc])\n",
    "\n",
    "loaded_model = load_model('tfrnn12_1.h5')\n",
    "print('acc : ', loaded_model.evaluate(x_test, y_test)[1])\n",
    "print('loss : ', loaded_model.evaluate(x_test, y_test)[0])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YYynGKe47nk6"
   },
   "source": [
    "# 시각화\n",
    "vloss = history.history['val_loss']\n",
    "loss = history.history['loss']\n",
    "x_len = np.arange(len(loss))\n",
    "plt.plot(x_len, vloss, marker='+', c='black', label='val_loss')\n",
    "plt.plot(x_len, loss, marker='o', c='red', label='loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Hr4HBkaGCFq1"
   },
   "source": [
    "import re\n",
    "\n",
    "def sentiment_predict(new_sentence):\n",
    "  new_sentence = re.sub('[^0-9a-zA-Z ]', '', new_sentence).lower()\n",
    "\n",
    "  # 정수 인코딩\n",
    "  encoded = []\n",
    "  for word in new_sentence.split():\n",
    "    # 단어 집합의 크기를 10,000으로 제한.\n",
    "    try :\n",
    "      if word_to_index[word] <= 10000:\n",
    "        encoded.append(word_to_index[word]+3)\n",
    "      else:\n",
    "        encoded.append(2)   # 10,000 이상의 숫자는 <unk> 토큰으로 취급.\n",
    "    except KeyError:\n",
    "      encoded.append(2)   # 단어 집합에 없는 단어는 <unk> 토큰으로 취급.\n",
    "\n",
    "  pad_new = pad_sequences([encoded], maxlen = max_len) # 패딩\n",
    "  \n",
    "  # 예측하기\n",
    "  score = float(loaded_model.predict(pad_new)) \n",
    "  if(score > 0.5):\n",
    "    print(\"{:.2f}% 확률로 긍정!.\".format(score * 100))\n",
    "  else:\n",
    "    print(\"{:.2f}% 확률로 부정!\".format((1 - score) * 100))\n",
    "\n",
    "\n",
    "# 긍/부정 분류 예측\n",
    "temp_str = \"This movie was just way too overrated. The fighting was not professional and in slow motion.\"\n",
    "sentiment_predict(temp_str)\n",
    "\n",
    "temp_str = \" I was lucky enough to be included in the group to see the advanced screening in Melbourne on the 15th of April, 2012. And, firstly, I need to say a big thank-you to Disney and Marvel Studios.\"\n",
    "sentiment_predict(temp_str)"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}