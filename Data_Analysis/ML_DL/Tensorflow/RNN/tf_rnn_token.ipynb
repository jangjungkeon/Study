{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 0, 'cat': 1, 'say': 2, 'on': 3, 'the': 4, 'mat.': 5, 'dog': 6, 'ate': 7, 'my': 8, 'homework.': 9}\n",
      "\n",
      "[[1, 2, 3, 4, 1, 5], [1, 6, 7, 8, 9]]\n",
      "{'the': 1, 'cat': 2, 'say': 3, 'on': 4, 'mat': 5, 'dog': 6, 'ate': 7, 'my': 8, 'homework': 9}\n"
     ]
    }
   ],
   "source": [
    "# 문자열 토큰화 + LSTM 감성분류\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "samples = ['The cat say on the mat.', 'The dog ate my homework.']    # list type\n",
    "\n",
    "# token 처리 1\n",
    "token_index = {}\n",
    "for sam in samples:\n",
    "  for word in sam.split(sep=' '):\n",
    "    if word not in token_index:\n",
    "      #print(word)\n",
    "      token_index[word] = len(token_index)\n",
    "\n",
    "print(token_index)    # {'The': 0, 'cat': 1, 'say': 2, 'on': 3, 'the': 4, 'mat.': 5, 'dog': 6, 'ate': 7, 'my': 8, 'homework.': 9}\n",
    "\n",
    "print()\n",
    "# token 처리 2 - word index\n",
    "tokenizer = Tokenizer()    # Tokenizer(num_words=5), 빈도가 높은 일부(여기서는 5개) 자료만 자료에 참여\n",
    "tokenizer.fit_on_texts(samples)\n",
    "token_seq = tokenizer.texts_to_sequences(samples)\n",
    "print(token_seq)      # [[1, 2, 3, 4, 1, 5], [1, 6, 7, 8, 9]]\n",
    "print(tokenizer.word_index)         # {'the': 1, 'cat': 2, 'say': 3, 'on': 4, 'mat': 5, 'dog': 6, 'ate': 7, 'my': 8, 'homework': 9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 1. 1. 1. 1.]]\n",
      "OrderedDict([('the', 3), ('cat', 1), ('say', 1), ('on', 1), ('mat', 1), ('dog', 1), ('ate', 1), ('my', 1), ('homework', 1)])\n",
      "2\n",
      "defaultdict(<class 'int'>, {'say': 1, 'on': 1, 'the': 2, 'mat': 1, 'cat': 1, 'my': 1, 'homework': 1, 'dog': 1, 'ate': 1})\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 6 is out of bounds for axis 1 with size 6",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-4-6dd8f4d0a0c4>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mutils\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mto_categorical\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 9\u001B[0;31m \u001B[0mtoken_seq\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mto_categorical\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtoken_seq\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_classes\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m6\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     10\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtoken_seq\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/setup/anaconda3/envs/python_pro2net/lib/python3.8/site-packages/tensorflow/python/keras/utils/np_utils.py\u001B[0m in \u001B[0;36mto_categorical\u001B[0;34m(y, num_classes, dtype)\u001B[0m\n\u001B[1;32m     76\u001B[0m   \u001B[0mn\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     77\u001B[0m   \u001B[0mcategorical\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzeros\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_classes\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 78\u001B[0;31m   \u001B[0mcategorical\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     79\u001B[0m   \u001B[0moutput_shape\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0minput_shape\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mnum_classes\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     80\u001B[0m   \u001B[0mcategorical\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcategorical\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutput_shape\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mIndexError\u001B[0m: index 6 is out of bounds for axis 1 with size 6"
     ]
    }
   ],
   "source": [
    "# mode='count' 등장 횟수, 'tfidf', 'freq' : 빈도수, 'binary' : 있으면 1없으면 0\n",
    "token_mat = tokenizer.texts_to_matrix(samples, mode='binary')\n",
    "print(token_mat)\n",
    "print(tokenizer.word_counts)\n",
    "print(tokenizer.document_count)\n",
    "print(tokenizer.word_docs)\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "token_seq = to_categorical(token_seq[1], num_classes=6)\n",
    "print(token_seq)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'너무': 1, '재미있어요': 2, '또': 3, '보고': 4, '싶어요': 5, '참': 6, '잘': 7, '만든': 8, '영화네요': 9, '친구에게': 10, '추천할래요': 11, '배우가': 12, '멋져요': 13, '별로에요': 14, '최악이에요': 15, '연기가': 16, '어색하네요': 17, '시간만': 18, '버렸어요': 19, '지루하고': 20, '재미없어요': 21}\n",
      "토큰화 결과 :  [[1, 2], [3, 4, 5], [6, 7, 8, 9], [10, 11], [12, 13], [14], [15], [16, 17], [18, 19], [20, 21]]\n",
      "[[ 0  0  1  2]\n",
      " [ 0  3  4  5]\n",
      " [ 6  7  8  9]\n",
      " [ 0  0 10 11]\n",
      " [ 0  0 12 13]\n",
      " [ 0  0  0 14]\n",
      " [ 0  0  0 15]\n",
      " [ 0  0 16 17]\n",
      " [ 0  0 18 19]\n",
      " [ 0  0 20 21]]\n"
     ]
    }
   ],
   "source": [
    "# 초간단 영화 리뷰 자료로 감성분석\n",
    "import numpy as np\n",
    "docs = ['너무 재미있어요', '또 보고 싶어요', '참 잘 만든 영화네요', '친구에게 추천할래요', '배우가 멋져요',\n",
    "        '별로에요', '최악이에요', '연기가 어색하네요', '시간만 버렸어요', '지루하고 재미없어요']\n",
    "classes = np.array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(docs)\n",
    "print(token.word_index)\n",
    "\n",
    "x = token.texts_to_sequences(docs)\n",
    "print('토큰화 결과 : ', x)\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "padded_x = pad_sequences(x, 4)    # 패딩 : 서로 다른 길이의 데이터를 가장 긴 데이터의 길이로 맞춘다.\n",
    "# 병렬 연산을 위해서 여러 문장의 길이를 임의로 동잃하게 맞춰주는 작업이 필요.\n",
    "print(padded_x)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 모델\n",
    "word_size = len(token.word_index) + 1       # embedding에서는 전체 word_size + 1을 해줘야한다.\n",
    "print(word_size)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Embedding, LSTM\n",
    "model = Sequential()\n",
    "model.add(Embedding(word_size, output_dim=8 , input_length=4))\n",
    "model.add(LSTM(32, activation='tanh'))\n",
    "model.add(Flatten())    # FC Layer\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_c'\n",
    "                                     'rossentropy', metrics=['accuracy'])\n",
    "model.fit(padded_x, classes, epochs=10, verbose=1)\n",
    "print('acc : ',  model.evaluate(padded_x, classes)[1])\n",
    "print('pred : ', model.predict(padded_x).flatten())\n",
    "print('real : ', classes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-2419e99e",
   "language": "python",
   "display_name": "PyCharm (Study)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}