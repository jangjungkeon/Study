{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jamotools\r\n",
      "  Downloading jamotools-0.1.10-py2.py3-none-any.whl (13 kB)\r\n",
      "Collecting future\r\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\r\n",
      "\u001B[K     |████████████████████████████████| 829 kB 318 kB/s eta 0:00:01\r\n",
      "\u001B[?25hRequirement already satisfied: six in /Users/jk/setup/anaconda3/envs/python_pro2net/lib/python3.8/site-packages (from jamotools) (1.15.0)\r\n",
      "Requirement already satisfied: numpy in /Users/jk/setup/anaconda3/envs/python_pro2net/lib/python3.8/site-packages (from jamotools) (1.19.5)\r\n",
      "Building wheels for collected packages: future\r\n",
      "  Building wheel for future (setup.py) ... \u001B[?25l-\b \b\\\b \b|\b \b/\b \b-\b \bdone\r\n",
      "\u001B[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491059 sha256=3b254ed8a9a9f8abe505de22d8557a2261de2a245ae6b7eebd6bed2ac05644aa\r\n",
      "  Stored in directory: /Users/jk/Library/Caches/pip/wheels/8e/70/28/3d6ccd6e315f65f245da085482a2e1c7d14b90b30f239e2cf4\r\n",
      "Successfully built future\r\n",
      "Installing collected packages: future, jamotools\r\n",
      "Successfully installed future-0.18.2 jamotools-0.1.10\r\n"
     ]
    }
   ],
   "source": [
    "!pip install jamotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import jamotools\n",
    "\n",
    "path_to_file = tf.keras.utils.get_file('toji.txt', 'https://raw.githubusercontent.com/pykwon/etc/master/rnn_test_toji.txt')\n",
    "# 데이터 로드 및 확인. encoding 형식 utf-8\n",
    "train_text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "# 텍스트 길이 확인\n",
    "print('Length of text: {} characters'.format(len(train_text)))\n",
    "print()\n",
    "\n",
    "# 한글 텍스트를 자모 단위로 분리, 한자 등에는 영향 x\n",
    "s = train_text[:100]\n",
    "s_split = jamotools.split_syllables(s)\n",
    "print(s_split)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 자모 결합 테스트\n",
    "s2 = jamotools.join_jamos(s_split)\n",
    "print(s2)\n",
    "print(s == s2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 자모 토큰화\n",
    "# 텍스트를 자모 단위로 split. 시간이 좀 걸림.\n",
    "train_text_X = jamotools.split_syllables(train_text)\n",
    "vocab = sorted(set(train_text_X))\n",
    "vocab.append('UNK')\n",
    "print ('{} unique characters'.format(len(vocab)))\n",
    "\n",
    "# vocab list를 숫자로 맵핑, 반대도\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in train_text_X])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 토큰 데이터 확인\n",
    "print(train_text_X[:20])\n",
    "print(text_as_int[:20])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 학습 데이터세트 생성\n",
    "seq_length = 80\n",
    "examples_per_epoch = len(text_as_int) // seq_length\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "char_dataset = char_dataset.batch(seq_length+1, drop_remainder=True)    # drop_remainder 쌓아주고 있긴한데, 남는 부분이 있으면 제거하겠다.\n",
    "for item in char_dataset.take(1):\n",
    "    print(idx2char[item.numpy()])\n",
    "    print(item.numpy())\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    return [chunk[:-1], chunk[-1]]\n",
    "\n",
    "train_dataset = char_dataset.map(split_input_target)\n",
    "for x,y in train_dataset.take(3):\n",
    "    print(idx2char[x.numpy()])\n",
    "    print(x.numpy())\n",
    "    print(idx2char[y.numpy()])\n",
    "    print(y.numpy())\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = examples_per_epoch // BATCH_SIZE\n",
    "BUFFER_SIZE = 5000\n",
    "\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 자소 단위 생성 모델 정의\n",
    "total_chars = len(vocab)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(total_chars, 100, input_length=seq_length),\n",
    "    tf.keras.layers.LSTM(units=400, activation='tanh'),\n",
    "    tf.keras.layers.Dense(total_chars, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 자소 단위 생성 모델 학습\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def testmodel(epoch, logs):\n",
    "    if epoch % 5 != 0 and epoch != 99:\n",
    "        return\n",
    "\n",
    "    test_sentence = train_text[:48]\n",
    "    test_sentence = jamotools.split_syllables(test_sentence)\n",
    "\n",
    "    next_chars = 300\n",
    "    for _ in range(next_chars):\n",
    "        test_text_X = test_sentence[-seq_length:]\n",
    "        test_text_X = np.array([char2idx[c] if c in char2idx else char2idx['UNK'] for c in test_text_X])\n",
    "        test_text_X = pad_sequences([test_text_X], maxlen=seq_length, padding='pre', value=char2idx['UNK'])\n",
    "\n",
    "        output_idx = model.predict_classes(test_text_X)\n",
    "        test_sentence += idx2char[output_idx[0]]\n",
    "\n",
    "    print()\n",
    "    print(jamotools.join_jamos(test_sentence))\n",
    "    print()\n",
    "\n",
    "testmodelcb = tf.keras.callbacks.LambdaCallback(on_epoch_end=testmodel)\n",
    "\n",
    "history = model.fit(train_dataset.repeat(), epochs=50, steps_per_epoch=steps_per_epoch, callbacks=[testmodelcb], verbose=2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.save(\"rnnmodel2.hdf5\")\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "test_sentence = '최참판댁 사랑은 무인지경처럼 적막하다'\n",
    "test_sentence = jamotools.split_syllables(test_sentence)\n",
    "\n",
    "next_chars = 5000\n",
    "for _ in range(next_chars):\n",
    "    test_text_X = test_sentence[-seq_length:]\n",
    "    test_text_X = np.array([char2idx[c] if c in char2idx else char2idx['UNK'] for c in test_text_X])\n",
    "    test_text_X = pad_sequences([test_text_X], maxlen=seq_length, padding='pre', value=char2idx['UNK'])\n",
    "\n",
    "    output_idx = model.predict_classes(test_text_X)\n",
    "    test_sentence += idx2char[output_idx[0]]\n",
    "\n",
    "\n",
    "print(jamotools.join_jamos(test_sentence))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-2419e99e",
   "language": "python",
   "display_name": "PyCharm (Study)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}